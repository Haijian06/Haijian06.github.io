# X-Chip与主流芯片的推理场景性能对比

*Author: 01.ai* 
*Date: 2024-07-23*

## 背景

随着 AGI/LLM 等大模型在各业务场景加速落地，今年是算力资源消耗重心从大模型训练向大模型推理转移的元年。我们判断未来超过 80% 的算力消耗将会由大模型推理承担。由于芯片研制流程较长，市场流通芯片通常落后市场需求 3 年左右，当前市场上没有一款合适性价比的推理芯片。这是因为大模型推理场景对显存容量和显存带宽非常敏感，对算力相对不敏感。当前主流芯片的显存和算力配比都是面向训练场景设计的，导致用于推理场景性价比不如人意。我们找到了一款消费级芯片 x-chip，显存和算力配比非常利于推理场景，和主流芯片比具有非常优秀的性价比。

本报告是 x-chip 和主流芯片推理场景对比测试。覆盖了 x-chip 和主流芯片的硬件参数和主流模型推理性能对比。本报告同时提供 Yi-34B/LLama3-70B/Yi-Large(132B) 的推理场景性能分析。需要注意的是，千亿参数稠密和万亿参数稀疏模型将会是未来 3-5 年的推理和微调门槛，34B/70B 性能数据仅作参考。

> [https://www.semianalysis.com/p/nvidia-blackwell-perf-tco-analysis](https://www.semianalysis.com/p/nvidia-blackwell-perf-tco-analysis)
>
> Larger than 100 billion parameter models will be the new norm for “small model” fine tuning and inference, and larger than 1 trillion parameter sparse models will be the norm among large models.

## x-chip 和主流芯片硬件参数对比

| GPU    | 显存量 | 显存带宽 | 卡间通信 (双向) | BFP16算力 | FP8算力       |
|--------|--------|----------|-----------------|-----------|---------------|
| A800   | 80GB   | 1935GB/s | NVLINK 400GB/s  | 312 TFLOPS| N/A           |
| H20    | 96GB   | 4TB/s    | NVLINK 900GB/s  | 148 TFLOPS| 296 TFLOPS    |
| 4090   | 24GB   | 1 TB/s   | 20-40GB/s       | 165 TFLOPS| Undisclosed   |
| **x-chip** | **48GB**   | 1 TB/s   | --              | --        | ~400TFLOPs    |

## 性能测试

性能测试针对 Yi-34B/Llama3-70B/Yi-Large 三个模型，包含在线场景（TTFT< 10 秒）和离线场景（最大吞吐）。在这六个场景下分别对比了 x-chip 和英伟达 4090/A800/H20 三款芯片的延迟和吞吐。

### Yi-34B

#### 在线场景

<table>
  <tr>
    <th>数据类型</th>
    <th>输入长度</th>
    <th>推理引擎</th>
    <th>GPU</th>
    <th>QPM</th>
    <th>TTFT</th>
  </tr>
  <tr>
    <td rowspan="4">BFP16</td>
    <td rowspan="2">4K</td>
    <td rowspan="4">vllm-v0.4.2</td>
    <td>4 * 4090</td>
    <td>14.4</td>
    <td>5.90 s</td>
  </tr>
  <tr>
    <td>4 * x-chip</td>
    <td><strong>35.4</strong></td>
    <td>6.14 s</td>
  </tr>
  <tr>
    <td rowspan="2">8K</td>
    <td>4 * 4090</td>
    <td>4.2</td>
    <td>5.86 s</td>
  </tr>
  <tr>
    <td>4 * x-chip</td>
    <td>14.4</td>
    <td>10.21 s</td>
  </tr>
</table>
  

#### 离线场景

<table>
  <tr>
    <th>数据类型</th>
    <th>输入长度</th>
    <th>推理引擎</th>
    <th>GPU</th>
    <th>QPM</th>
  </tr>
  <tr>
    <td rowspan="4">BFP16</td>
    <td rowspan="2">4K</td>
    <td rowspan="4">vllm-v0.4.2</td>
    <td>4 * 4090</td>
    <td>16.8</td>
  </tr>
  <tr>
    <td>4 * x-chip</td>
    <td><strong>39</strong></td>
  </tr>
  <tr>
    <td rowspan="2">8K</td>
    <td>4 * 4090</td>
    <td>5.4</td>
  </tr>
  <tr>
    <td>4 * x-chip</td>
    <td>15.6</td>
  </tr>
</table>

### llama3-70B-Instruct

#### 在线场景
<table>
  <tr>
    <th>数据类型</th>
    <th>输入长度</th>
    <th>推理引擎</th>
    <th>GPU</th>
    <th>QPM</th>
    <th>TTFT</th>
  </tr>
  <tr>
    <td rowspan="4">BFP16</td>
    <td rowspan="2">4K</td>
    <td rowspan="4">vllm-v0.4.2</td>
    <td>4 * A800</td>
    <td>42.6</td>
    <td>2.23 s</td>
  </tr>
  <tr>
    <td>4 * x-chip</td>
    <td>16.2</td>
    <td>9.18 s</td>
  </tr>
  <tr>
    <td rowspan="2">8K</td>
    <td>4 * A800</td>
    <td>13.8</td>
    <td>2.47 s</td>
  </tr>
  <tr>
    <td>4 * x-chip</td>
    <td>4.8</td>
    <td>13.28 s</td>
  </tr>
  <tr>
    <td rowspan="4">FP8</td>
    <td rowspan="2">4K</td>
    <td rowspan="4">vllm-v0.4.2</td>
    <td>4 * A800</td>
    <td><strong>N/A</strong></td>
    <td><strong>N/A</strong></td>
  </tr>
  <tr>
    <td>4 * x-chip</td>
    <td>21.6</td>
    <td>5.58 s</td>
  </tr>
  <tr>
    <td rowspan="2">8K</td>
    <td>4 * A800</td>
    <td><strong>不支持</strong></td>
    <td><strong>不支持</strong></td>
  </tr>
  <tr>
    <td>4 * x-chip</td>
    <td>9</td>
    <td>8.29 s</td>
  </tr>
</table>

#### 离线场景

<table>
  <thead>
    <tr>
      <th>数据类型</th>
      <th>输入长度</th>
      <th>推理引擎</th>
      <th>GPU</th>
      <th>QPM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="4">BFP16</td>
      <td rowspan="2">4K</td>
      <td rowspan="4">vllm-v0.4.2</td>
      <td>4 * A800</td>
      <td>50.4</td>
    </tr>
    <tr>
      <td>4 * x-chip</td>
      <td>16.8</td>
    </tr>
    <tr>
      <td rowspan="2">8K</td>
      <td>4 * A800</td>
      <td>18.6</td>
    </tr>
    <tr>
      <td>4 * x-chip</td>
      <td>6.6</td>
    </tr>
    <tr>
      <td rowspan="4">FP8</td>
      <td rowspan="2">4K</td>
      <td rowspan="4">vllm-v0.4.2</td>
      <td>4 * A800</td>
      <td><strong>N/A</strong></td>
    </tr>
    <tr>
      <td>4 * x-chip</td>
      <td>27</td>
    </tr>
    <tr>
      <td rowspan="2">8K</td>
      <td>4 * A800</td>
      <td><strong>N/A</strong></td>
    </tr>
    <tr>
      <td>4 * x-chip</td>
      <td>11.4</td>
    </tr>
  </tbody>
</table>

### yi-large(yi-132b)

#### 在线场景

<table>
  <thead>
    <tr>
      <th>数据类型</th>
      <th>输入长度</th>
      <th>GPU</th>
      <th>推理引擎</th>
      <th>QPM</th>
      <th>TTFT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="6">BFP16</td>
      <td rowspan="3">4K</td>
      <td>8 * A800</td>
      <td rowspan="6">vllm-v0.4.2</td>
      <td>40.8</td>
      <td>6.68 s</td>
    </tr>
    <tr>
      <td>8 * x-chip</td>
      <td>11.4</td>
      <td>10.83 s</td>
    </tr>
    <tr>
      <td>8 * H20</td>
      <td>35.4</td>
      <td>9.25 s</td>
    </tr>
    <tr>
      <td rowspan="3">8K</td>
      <td>8 * A800</td>
      <td>15.6</td>
      <td>4.61 s</td>
    </tr>
    <tr>
      <td>8 * x-chip</td>
      <td>4.2</td>
      <td>19.05 s</td>
    </tr>
    <tr>
      <td>8 * H20</td>
      <td>11.4</td>
      <td>4.85 s</td>
    </tr>
    <tr>
      <td rowspan="6">FP8</td>
      <td rowspan="3">4K</td>
      <td>8 * x-chip</td>
      <td rowspan="6">vllm-v0.4.2</td>
      <td>16.2</td>
      <td>13.32 s</td>
    </tr>
    <tr>
      <td>8 * A800</td>
      <td>N/A</td>
      <td>N/A</td>
    </tr>
    <tr>
      <td>8 * H20</td>
      <td>46.2</td>
      <td>3.51 s</td>
    </tr>
    <tr>
      <td rowspan="3">8K</td>
      <td>8 * x-chip</td>
      <td>6.6</td>
      <td>22.45 s</td>
    </tr>
    <tr>
      <td>8 * A800</td>
      <td>N/A</td>
      <td>N/A</td>
    </tr>
    <tr>
      <td>8 * H20</td>
      <td>16.2</td>
      <td>2.78 s</td>
    </tr>
  </tbody>
</table>

#### 离线场景
<table>
  <thead>
    <tr>
      <th>数据类型</th>
      <th>输入长度</th>
      <th>GPU</th>
      <th>推理引擎</th>
      <th>QPM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="6">BFP16</td>
      <td rowspan="3">4K</td>
      <td>8 * A800</td>
      <td rowspan="6">vllm-v0.4.2</td>
      <td>45.6</td>
    </tr>
    <tr>
      <td>8 * x-chip</td>
      <td>12.6</td>
    </tr>
    <tr>
      <td>8 * H20</td>
      <td>37.2</td>
    </tr>
    <tr>
      <td rowspan="3">8K</td>
      <td>8 * A800</td>
      <td>16.8</td>
    </tr>
    <tr>
      <td>8 * x-chip</td>
      <td>5.4</td>
    </tr>
    <tr>
      <td>8 * H20</td>
      <td>14.5</td>
    </tr>
    <tr>
      <td rowspan="4">FP8</td>
      <td rowspan="2">4K</td>
      <td>8 * x-chip</td>
      <td rowspan="4">vllm-v0.4.2</td>
      <td>17.4</td>
    </tr>
    <tr>
      <td>8 * H20</td>
      <td>53.8</td>
    </tr>
    <tr>
      <td rowspan="2">8K</td>
      <td>8 * x-chip</td>
      <td>6.6</td>
    </tr>
    <tr>
      <td>8 * H20</td>
      <td>19.2</td>
    </tr>
  </tbody>
</table>

## 稳定性表现

测试过程中，x-chip 温度基本稳定在 60 度上下，未发生 SBE/DBE 等 ECC 错误，稳定性和市售主流芯片保持一致。

## 测试结论

1. Yi-34B 模型下，x-chip 与 4090 延迟相当，吞吐量显著优于 4090；
2. LLama3-70B 模型下，x-chip 延迟是 A800 的 4-5 倍，吞吐量显著低于 A800；
3. Yi-Large(132B) 模型下，x-chip 与 H20 延迟相当，吞吐量显著低于 H20，远低于 A800；

<p align="center">
  <img src="../.././assets/images/blog-6.img/X-chip.png? raw=true" alt="X-chip.png" width="600"/>
</p>
